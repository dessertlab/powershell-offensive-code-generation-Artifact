{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this Notebook the is needed only the library \"Transformers\" but we reccomend to install all the requirements present into the gitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Inference\n",
    "\n",
    "This notebook provides an overview of the code generation capabilities of the models that have been pre-trained and fine-tuned on offensive PowerShell code generation. It outlines two methods of utilising these models: the first method employs the API provided by HuggingFace, which involves the models and dataset being uploaded; the second method downloads the models directly, requiring a minimum of 3 GB of free space on the user's disk.\n",
    "Furthermore, the models may be tested using the graphical user interface (GUI) provided on the HuggingFace repository. This repository can be accessed via the following link: https://huggingface.co/collections/dessertlab/the-power-of-words-generating-powershell-attacks-from-66223c3e6cd34bb31ce38a69.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Inference\n",
    "In order to utilise the HuggingFace API for inference, it is necessary for the user to be logged into HuggingFace. Should the user be unwilling to log in, they may alternatively download the requisite models via the code provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "READ_TOKEN = \"[HF TOKEN]\"                                                                                       # insert your HF token of use the CLI HuggingFace LogIn\n",
    "MODEL = \"dessertlab/offensive-powershell-CodeGPT-small\"\n",
    "MODEL = \"dessertlab/offensive-powershell-codegen-350M-multi\"\n",
    "MODEL = \"dessertlab/offensive-powershell-codet5p-220m-py\"                                                       # Choose one of the three models \n",
    "API_URL = f\"https://api-inference.huggingface.co/models/{MODEL}\"\n",
    "HEADERS = {\"Authorization\": f\"Bearer {READ_TOKEN}\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=HEADERS, json=payload)\n",
    "\treturn response.json()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    if(\"codet5p\" in MODEL):\n",
    "        output = query({\n",
    "            \"inputs\": \"System Information with WMI.\",\n",
    "            \"options\": {\"wait_for_model\": True}\n",
    "        })\n",
    "    else: #codegpt and codegen\n",
    "        output = query({\n",
    "            \"inputs\": \"System Information with WMI. <s>\",\n",
    "            \"parameters\": {\"return_full_text\": False},\n",
    "            \"options\": {\"wait_for_model\": True}\n",
    "        })\n",
    "    \n",
    "    print(output)\n",
    "    \n",
    "    #skip undecodable characters\n",
    "    output[0]['generated_text'] = output[0]['generated_text'].replace('ï¿½', ' ')\n",
    "\n",
    "    with open('output.txt', 'w') as f:\n",
    "        f.write(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In place inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CodeT5 plus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated code: $code = {Invoke-Expression -Command \"calc.exe\"}; Invoke-Command\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM  \n",
    "checkpoint_gpt = \"dessertlab/offensive-powershell-CodeGPT-small\"\n",
    "MODEL = \"dessertlab/offensive-powershell-codegen-350M-multi\"\n",
    "checkpoint_t5p = \"dessertlab/offensive-powershell-codet5p-220m-py\"  \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=checkpoint_t5p)\n",
    "model_t5p = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name_or_path=checkpoint_t5p)\n",
    "\n",
    "\n",
    "prompt = input(\"Please insert your prompt\")\n",
    "prompt = 'Execute PowerShell code to open the Windows calculator using remote command execution' #SAMPLE PROMPT\n",
    "tokenized_outputs = tokenizer(\n",
    "    prompt,\n",
    "    max_length = 512, \n",
    "    truncation = True,  \n",
    "    padding = 'max_length', \n",
    "    return_tensors = 'pt', \n",
    ")\n",
    "\n",
    "output = model_t5p.generate(tokenized_outputs['input_ids'],\n",
    "                            max_length=20, \n",
    "                            num_beams=5,    \n",
    "                            early_stopping=True)\n",
    "\n",
    "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated code:\", decoded_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CodeGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated code: Execute PowerShell code to open the Windows calculator using remote command execution.  $code = {Invoke-Expression -Command \"calc.exe\"}; Invoke-Command -ScriptBlock $code\n",
      "Generated Code without Prompt:\n",
      "$code = {Invoke-Expression -Command \"calc.exe\"}; Invoke-Command -ScriptBlock $code\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "checkpoint_gpt = \"dessertlab/offensive-powershell-CodeGPT-small\"\n",
    "checkpoint_gen = \"dessertlab/offensive-powershell-codegen-350M-multi\"\n",
    "checkpoint_t5p = \"dessertlab/offensive-powershell-codet5p-220m-py\"  \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=checkpoint_gpt)\n",
    "model_gpt = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=checkpoint_gpt)\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "prompt = input(\"Please insert your prompt\")\n",
    "prompt = 'Execute PowerShell code to open the Windows calculator using remote command execution. ' #SAMPLE PROMPT\n",
    "\n",
    "tokenized_outputs = tokenizer(\n",
    "    prompt,\n",
    "    max_length = 200, \n",
    "    truncation = True,  \n",
    "    padding = 'max_length',\n",
    "    return_tensors = 'pt', \n",
    ")\n",
    "\n",
    "output = model_gpt.generate(tokenized_outputs['input_ids'],   \n",
    "                            max_length=512,\n",
    "                            num_return_sequences=1\n",
    "                        )\n",
    "\n",
    "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Output cleaning\n",
    "\n",
    "prompt_position = decoded_output.find(prompt)\n",
    "\n",
    "# If the prompt is found, slice the generated text to remove it\n",
    "if prompt_position != -1:\n",
    "    generated_part = decoded_output[prompt_position + len(prompt):]\n",
    "else:\n",
    "    generated_part = decoded_output  \n",
    "\n",
    "print(\"Generated Code without Prompt:\")\n",
    "print(generated_part.strip())  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
